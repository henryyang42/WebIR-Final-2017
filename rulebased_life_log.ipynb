{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entities\n",
    "\n",
    "- `names_twrsj.txt`: names from 台灣人士鑑\n",
    "\n",
    "- `place_wiki.csv`: place from wiki\n",
    "\n",
    "- `relatives.txt`: 到處收集的各種對人的稱呼\n",
    "\n",
    "*__不使用 `names.csv` 和 `place.csv`__* 因為等一下用dictionary直接分就好了啊 OAO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names, relatives, places, time_trigger = set(), set(), set(), set()\n",
    "\n",
    "with open('data/names_twrsj.txt') as f:\n",
    "    names.update(name.strip() for name in f)\n",
    "\n",
    "with open('data/relatives.txt') as f:\n",
    "    relatives.update(name.strip() for name in f)\n",
    "\n",
    "with open('data/place_wiki.csv') as f:\n",
    "    places.update(place.strip() for place in f)\n",
    "\n",
    "\n",
    "nums = ['一', '二', '三', '四', '五', '六', '七', '八', '九', '十', '十一', '十二', '兩']\n",
    "time_trigger.update(['下午', '早晨', '晚上', '早上', '午夜', '今天', '明天'])\n",
    "time_trigger.update(num + '點' for num in nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Japanese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import re\n",
    "diaries = pd.read_csv('data/diary_corpus.csv').fillna('')\n",
    "diaries = diaries[diaries.author =='楊基振日記']\n",
    "days = ['星期一', '星期二', '星期三', '星期四', '星期五', '星期六', '星期日', '星期天']\n",
    "out  = []\n",
    "pat = re.compile('。')\n",
    "for i in diaries.index:\n",
    "    content = diaries.content[i].strip()\n",
    "    title = diaries.title[i]\n",
    "    for day in days:\n",
    "        ind = content.find(day+'\\n')\n",
    "        if ind > 0:\n",
    "            content = content[ind+3:]\n",
    "            break\n",
    "    out += [title, sentence.strip() for sentence in pat.split(content) if sentence.strip()]\n",
    "len(out)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Time Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "# 沒事不要再做一次 這個很久\n",
    "'''\n",
    "from jseg.jieba import Jieba\n",
    "segmenter = Jieba()\n",
    "'''\n",
    "from PyCCS import ckip\n",
    "segmenter = ckip\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def entry2frame(entry):\n",
    "    title, sentence = entry\n",
    "    if any(trigger in sentence for trigger in time_trigger):\n",
    "        result = segmenter.seg(sentence)\n",
    "        seg, pos = zip(*result.raw)\n",
    "        return {'time': title, \n",
    "                'content': sentence,\n",
    "                'segment': seg, \n",
    "                'postag': pos}\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "filename = 'data/frame_seg.jsonl'\n",
    "pool = Pool()\n",
    "frames = pool.imap_unordered(entry2frame, out, chunksize=4)\n",
    "pool.close()\n",
    "with open(filename, 'a') as f:\n",
    "    f.seek(0)\n",
    "    f.truncate()\n",
    "    for i, frame in enumerate(frames, 1):\n",
    "        if frame is not None:\n",
    "            json.dump(frame, f, ensure_ascii=False)\n",
    "            print(file=f)\n",
    "        print('{:>5d}/{:>5d}'.format(i, len(out)), end='\\r')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Format\n",
    "\n",
    "- `Na`: 普通名詞\n",
    "\n",
    "- `Nb`: 專有名詞（含人名）\n",
    "\n",
    "- `Nha`: 人稱代名詞    \n",
    "\n",
    "- `Nc`: 位置\n",
    "\n",
    "- `Nd`: 時間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data/dict.txt.big', 'r') as f:\n",
    "    common_words = set(line.split()[0] for line in f)\n",
    "with open('data/frame_seg.jsonl', 'r') as f:\n",
    "    frames = [json.loads(frame) for frame in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "person_tag = 'Nb'\n",
    "place_tag = 'Nc'\n",
    "time_tag = 'Nd'\n",
    "\n",
    "person_tag_soft = ('Na', 'Nha')\n",
    "\n",
    "entities = 'relative_time place participant tokens'.split()\n",
    "for frame in frames:\n",
    "    frame.update({entity: [] for entity in entities})\n",
    "    sentence = zip(frame.pop('segment'), frame.pop('postag'))\n",
    "    \n",
    "    for idx, (token, pos) in enumerate(sentence):\n",
    "        label = 'O'\n",
    "        \n",
    "        if pos.startswith(time_tag) \\\n",
    "                or token in time_trigger:                \n",
    "            label = 'TIME'\n",
    "            frame['relative_time'].append({'token': token, 'index': idx})\n",
    "            \n",
    "            \n",
    "        elif pos.startswith(place_tag) \\\n",
    "                or token in places:                \n",
    "            label = 'PLACE'\n",
    "            frame['place'].append({'token': token, 'index': idx})\n",
    "            \n",
    "            \n",
    "        elif pos.startswith(person_tag) \\\n",
    "                or pos.startswith(person_tag_soft) and token     in names \\\n",
    "                or pos.startswith('N')             and token     in relatives \\\n",
    "                or pos.startswith('Na')            and token not in common_words:                    \n",
    "            label = 'PERSON'\n",
    "            frame['participant'].append({'token': token, 'index': idx})\n",
    "            \n",
    "            \n",
    "        frame['tokens'].append({'token': token, 'index': idx, 'pos': pos, 'label': label})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Frames to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4688\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('frames.jsonl', 'w') as f:\n",
    "    for frame in frames:\n",
    "        print(json.dumps(frame, ensure_ascii=False), file=f)\n",
    "# I still need json QQ\n",
    "with open('frames.json', 'w') as f:\n",
    "    print(json.dumps(frames, ensure_ascii=False, indent=2), file=f)\n",
    "print(len(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': '過了下午四點，帶母親、肇嘉嫂、淑英等孩子們到天壇遊覽',\n",
       " 'participant': [{'index': 6, 'token': '母親'},\n",
       "  {'index': 8, 'token': '肇嘉嫂'},\n",
       "  {'index': 10, 'token': '淑英'},\n",
       "  {'index': 12, 'token': '孩子們'}],\n",
       " 'place': [{'index': 14, 'token': '天壇'}],\n",
       " 'relative_time': [{'index': 2, 'token': '下午'}, {'index': 3, 'token': '四點'}],\n",
       " 'time': ' 1944年（民33年，34歲）   10月1日\\u3000日 ',\n",
       " 'tokens': [{'index': 0, 'label': 'O', 'pos': 'VCL', 'token': '過'},\n",
       "  {'index': 1, 'label': 'O', 'pos': 'Di', 'token': '了'},\n",
       "  {'index': 2, 'label': 'TIME', 'pos': 'Nd', 'token': '下午'},\n",
       "  {'index': 3, 'label': 'TIME', 'pos': 'Neu', 'token': '四點'},\n",
       "  {'index': 4, 'label': 'O', 'pos': 'COMMACATEGORY', 'token': '，'},\n",
       "  {'index': 5, 'label': 'O', 'pos': 'VC', 'token': '帶'},\n",
       "  {'index': 6, 'label': 'PERSON', 'pos': 'Na', 'token': '母親'},\n",
       "  {'index': 7, 'label': 'O', 'pos': 'PAUSECATEGORY', 'token': '、'},\n",
       "  {'index': 8, 'label': 'PERSON', 'pos': 'Na', 'token': '肇嘉嫂'},\n",
       "  {'index': 9, 'label': 'O', 'pos': 'PAUSECATEGORY', 'token': '、'},\n",
       "  {'index': 10, 'label': 'PERSON', 'pos': 'Nb', 'token': '淑英'},\n",
       "  {'index': 11, 'label': 'O', 'pos': 'Cab', 'token': '等'},\n",
       "  {'index': 12, 'label': 'PERSON', 'pos': 'Na', 'token': '孩子們'},\n",
       "  {'index': 13, 'label': 'O', 'pos': 'VCL', 'token': '到'},\n",
       "  {'index': 14, 'label': 'PLACE', 'pos': 'Nc', 'token': '天壇'},\n",
       "  {'index': 15, 'label': 'O', 'pos': 'VCL', 'token': '遊覽'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames[37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
